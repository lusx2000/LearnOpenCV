# OpenCV 视觉实战第二讲作业

- 自己找到1-2套不同的前景和背景图像，使用本文算法进行处理，看看效果

  对老师提供的代码做了一些修改，最终使用代码如下：

  ```c++
  #include <opencv2/opencv.hpp>
  #include <vector>
  using namespace std;
  using namespace cv;
  
  vector<Point> find_maximum_contour(Mat& src) {
      int max_contours = -1;
      vector<Point> max_contour;
      vector<vector<Point>> contours;
      findContours(src, contours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);
      for (const auto &contour : contours) {
          auto tmp_area = contourArea(contour);
          if (max_contours < tmp_area) {
              max_contour = contour;
          }
      }
      return max_contour;
  }
  
  
  //提高饱和度
  Mat EnhanceSaturation(Mat& temp)
  {
      Mat matDst;
      Mat Img_out(temp.size(), CV_32FC3);
      temp.convertTo(Img_out, CV_32FC3);
      Mat Img_in(temp.size(), CV_32FC3);
      temp.convertTo(Img_in, CV_32FC3);
      // define the iterator of the input image
      MatIterator_<Vec3f> inp_begin, inp_end;
      inp_begin=Img_in.begin<Vec3f>();
      inp_end =Img_in.end<Vec3f>();
      // define the iterator of the output image
      MatIterator_<Vec3f> out_begin, out_end;
      out_begin=Img_out.begin<Vec3f>();
      out_end =Img_out.end<Vec3f>();
      // increment (-100.0, 100.0)
      float Increment=50.0/100.0;   //饱和度参数调整
      float delta=0;
      float minVal, maxVal;
      float t1, t2, t3;
      float L,S;
      float alpha;
  
      for(; inp_begin!=inp_end; inp_begin++, out_begin++)
      {
          t1=(*inp_begin)[0];
          t2=(*inp_begin)[1];
          t3=(*inp_begin)[2];
  
          minVal=std::min(std::min(t1,t2),t3);
          maxVal=std::max(std::max(t1,t2),t3);
          delta=(maxVal-minVal)/255.0;
          L=0.5*(maxVal+minVal)/255.0;
          S=std::max(0.5*delta/L, 0.5*delta/(1-L));
  
          if (Increment>0)
          {
              alpha=max(S, 1-Increment);
              alpha=1.0/alpha-1;
              (*out_begin)[0]=(*inp_begin)[0]+((*inp_begin)[0]-L*255.0)*alpha;
              (*out_begin)[1]=(*inp_begin)[1]+((*inp_begin)[1]-L*255.0)*alpha;
              (*out_begin)[2]=(*inp_begin)[2]+((*inp_begin)[2]-L*255.0)*alpha;
          }
          else
          {
              alpha=Increment;
              (*out_begin)[0]=L*255.0+((*inp_begin)[0]-L*255.0)*(1+alpha);
              (*out_begin)[1]=L*255.0+((*inp_begin)[1]-L*255.0)*(1+alpha);
              (*out_begin)[2]=L*255.0+((*inp_begin)[2]-L*255.0)*(1+alpha);
  
          }
      }
      Img_out /=255;
      Img_out.convertTo(matDst,CV_8UC3,255);
  
      return matDst;
  }
  
  
  int main() {
      // Origin Images
      auto src = imread("/home/lusx/Pictures/merge/street2.jpg");
      resize(src, src, Size(src.cols/4, src.rows/4));
      auto cloud = imread("/home/lusx/Pictures/merge/cloud3.jpg");
      resize(cloud, cloud, src.size());
      // Temp Variables
      Mat hsv_image;
      vector<Mat> planes;
      // Separate to HSV
      cvtColor(src, hsv_image, COLOR_BGR2HSV);
      split(hsv_image, planes);
      // Equalization Value(亮度)
      equalizeHist(planes[2], planes[2]);
      merge(planes, hsv_image);
      inRange(hsv_image, Scalar(100, 43, 46), Scalar(124, 255, 255), hsv_image);
      erode(hsv_image, hsv_image, Mat());
      dilate(hsv_image, hsv_image, Mat());
      auto mask = hsv_image.clone();
      ///////////////////////////////////////////////////////
      ///////////////////////////////////////////////////////
  
      auto max_contour = find_maximum_contour(mask);
      auto max_rect = boundingRect(max_contour);
      if (max_rect.height == 0 || max_rect.width == 0) {
          max_rect = Rect(0, 0, mask.cols, mask.rows);
      }
      auto mat_dst = src.clone();
      mask = mask(max_rect);
      resize(cloud, cloud, max_rect.size());
      auto center = Point((max_rect.x + max_rect.width)/2, (max_rect.y + max_rect.height)/2);
      Mat clone;
      seamlessClone(cloud, src, mask, center, clone, NORMAL_CLONE);
      ///////////////////////////////////////////////////////
      ///////////////////////////////////////////////////////
  
      // Enhance Saturation
      bilateralFilter(clone, hsv_image, 5, 10.0, 2.0);
      cvtColor(hsv_image, hsv_image, COLOR_BGR2YCrCb);
      split(hsv_image, planes);
      equalizeHist(planes[0], planes[0]);
      merge(planes, hsv_image);
      cvtColor(hsv_image, hsv_image, COLOR_YCrCb2BGR);
      auto dst = EnhanceSaturation(hsv_image);
  
      imshow("before clone", src);
      imshow("after clone", dst);
      waitKey();
  
      imwrite("before2.jpg", src);
      imwrite("after2.jpg", dst);
  
      destroyAllWindows();
      return 0;
  }
  ```

  自己拍摄的图片：

  ![原图](/home/lusx/CLionProjects/cv_in_practice/cmake-build-debug/before.jpg)

![滤镜处理后](/home/lusx/CLionProjects/cv_in_practice/cmake-build-debug/after.jpg)

![before](/home/lusx/CLionProjects/cv_in_practice/cmake-build-debug/before2.jpg)

![after2](/home/lusx/CLionProjects/cv_in_practice/cmake-build-debug/after2.jpg)

可以发现都实现了预期的效果，但是第二张的右边部分的松树被部分当成了背景，导致最后融合结果较为不真实，个人推测是由于HSV颜色空间的蓝色范围太宽导致，如果将其进行微调，可能效果会更好。

- 对于二阶泊松融合的理解

  函数的梯度即函数的变化趋势。对图像融合来说，打个比方，如果要融合两个函数(图像)，可以记下要融合的图片的变化趋势(梯度)，让原图按照同样的趋势“生长”出要融合的部分，图像的衔接就会更加自然。

  函数的一阶导是函数的变化趋势(斜率)，那么二阶导就是变化趋势的趋势，如果我们能够保证原图和新图的变化趋势的趋势(二阶导)差距达到最小，那么它们的趋势变化也会达到最小，这样函数就不会有突兀的改变。

  上面的这段描述最后在Poission Image Editing 这篇论文中最后变为解如下的函数：

  $$\mathop{\min}\limits_{f}\iint_Ω\left|\nabla f-{\bf v}\right|^2 with \left. f \right| _{\partialΩ}=\left.f^*\right|_{\partialΩ}$$

  其中f就是最后融合的图像，f*是原来的图像，v就是要融合的图像，通过一系列的迭代优化技术，最后获得融合的结果图像f。

  *在融合过程中，其实并没有将要融合的图片“糊”上去，而是通过计算散度(也就是上文提到的二阶导，只不过这里是偏导)，让原图的待融合部分按照上式生成的规则算出这部分的像素值，最后获得融合后的图片*
